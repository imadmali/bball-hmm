---
title: "Tagging Basketball Plays with HMM"
author: "Imad Ali"
date: "8/5/2019"
output: 
  html_document:
    toc: true
    theme: cosmo
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rstan)
library(bayesplot)
rstan_options(auto_write = TRUE)
```

## Introduction

## Simple HMM Example

```{r fig.align='center', fig.height=8, fig.width=9}
hmm_data <- readRDS("../data/hmm_example_data.RDS")
z <- hmm_data$z
y <- hmm_data$y
par(mfrow=c(2,1))
plot(hmm_data$z, type="s",
     main = "Latent States",
     ylab = "State Value",
     xlab = "Time",
     ylim = c(0.5,2.5), yaxt = "n")
axis(2, 1:2, 1:2)
plot(hmm_data$y, type = "l",
     main = "Observed Output",
     ylab = "Observation Value",
     xlab = "Time")
y_plt <- hmm_data$y
y_plt[hmm_data$z==1] <- NA
lines(y_plt, lwd = 3)
legend("bottomright", c("State 1","State 2"), lty = c(1,1), lwd = c(1,3), cex = 0.8)
```

### Specifying the Model

```{stan eval=FALSE, output.var='hmm_example'}
data {
  int<lower=0> N;
  int<lower=0> K;
  real y[N];
}

parameters {
  simplex[K] theta[K];
  // real psi[K];
  positive_ordered[K] psi;
}

model {
  // priors
  target+= normal_lpdf(psi[1] | 3, 1);
  target+= normal_lpdf(psi[2] | 10, 1);
  // forward algorithm
  {
  real acc[K];
  real gamma[N, K];
  for (k in 1:K)
    // gamma[1, k] = log(phi[k, u[1]]);
    gamma[1, k] = normal_lpdf(y[1] | psi[k], 1);
  for (t in 2:N) {
    for (k in 1:K) {
      for (j in 1:K)
        // acc[j] = gamma[t-1, j] + log(theta[j, k]) + log(phi[k, u[t]]);
        acc[j] = gamma[t-1, j] + log(theta[j, k]) + normal_lpdf(y[t] | psi[k], 1);
      gamma[t, k] = log_sum_exp(acc);
    }
  }
  target += log_sum_exp(gamma[N]);
  }
}

generated quantities {
  int<lower=1,upper=K> y_star[N];
  real log_p_y_star;
  {
    int back_ptr[N, K];
    real best_logp[N, K];
    real best_total_logp;
    for (k in 1:K)
      // best_logp[1, k] = log(phi[k, u[1]]);
      best_logp[1, k] = normal_lpdf(y[1] | psi[k], 1);
    for (t in 2:N) {
      for (k in 1:K) {
        best_logp[t, k] = negative_infinity();
        for (j in 1:K) {
          real logp;
          // logp = best_logp[t-1, j] + log(theta[j, k]) + log(phi[k, u[t]]);
          logp = best_logp[t-1, j] + log(theta[j, k]) + normal_lpdf(y[t] | psi[k], 1);
          if (logp > best_logp[t, k]) {
            back_ptr[t, k] = j;
            best_logp[t, k] = logp;
          }
        }
      }
    }
    log_p_y_star = max(best_logp[N]);
    for (k in 1:K)
      if (best_logp[N, k] == log_p_y_star)
        y_star[N] = k;
    for (t in 1:(N - 1))
      y_star[N - t] = back_ptr[N - t + 1, y_star[N - t + 1]];
  }
}
```

### Fitting the Model

```{r results='hide'}
stan_data <- list(N = length(hmm_data$y),
                  K = 2,
                  y = hmm_data$y)
hmm_fit <- stan("../models/hmm_example.stan", data = stan_data, iter = 1e3, chains = 4)
```

### Post-estimation

#### Diagnostics

```{r}
samples <- as.matrix(hmm_fit)

psi_indx <- grep("^psi\\[", colnames(samples))
theta_indx <- grep("^theta\\[", colnames(samples))
y_star_indx <- grep("^y_star\\[", colnames(samples))

```

```{r fig.align='center', fig.width=6, fig.height=6}
mcmc_trace(as.array(hmm_fit), regex_pars = "^theta\\[")
```

```{r fig.align='center', fig.width=6, fig.height=3}
mcmc_trace(as.array(hmm_fit), regex_pars = "^psi\\[")
```

#### Predictions

```{r}
colMeans(samples[,theta_indx])
colMeans(samples[,psi_indx])

y_star <- colMeans(samples[,y_star_indx])
```

```{r fig.align='center', fig.width=8, fig.height=9}
# visualization
par(mfrow=c(2,1))
plot(hmm_data$z, type="s",
     main = "Latent States",
     ylab = "State Value",
     xlab = "Time",
     ylim = c(0.5,2.5), yaxt = "n")
axis(2, 1:2, 1:2)
points(y_star, cex = 0.5)
legend("bottomright", c("Actual","Predicted"), pch = c(NA,1), lty = c(1,NA), cex = 0.8)
plot(hmm_data$y, type = "l",
     main = "Observed Output",
     ylab = "Observation Value",
     xlab = "Time")
y_plt <- hmm_data$y
y_plt[hmm_data$z==1] <- NA
lines(y_plt, lwd = 3)
legend("bottomright", c("State 1","State 2"), lty = c(1,1), lwd = c(1,3), cex = 0.8)
```

## Tagging Drive Events

The goal here is to apply the methodology above to tag a drive in basketball. A drive occurs when a player dribbles the ball towards the hoop for a layup. We can translate a drive into two types of events that are happening over time until the layup is attempted,

* The player increases their speed.
* The player reduces the distance between himself and the basket.

The video below illustrates what a drive event looks like in the player tracking data. This drive possession was attributed to Zach LaVine in a Minnesota Timberwolves v Boston Celtics game (12/21/2015).

### Post-processing Data

Using the player tracking data we can construct metrics associated with our time series events. We define speed as distance over time and use  Euclidean distance to determine the player's distance from the hoop at each time step. Below is an illustration of Zach LaVine's distance from basket and speed metrics. Consistent with our interpretation of drive, notice how he decreases his distance from the basket as he increases his speed.

```{r fig.align='center', fig.height=8, fig.width=9}
drive_data <- readRDS("../data/evt140_0021500411.RDS")
par(mfrow = c(2,1))
plot(drive_data$game$lavine_dist, type = "l",
     xlab = "Time (25hz)", ylab = "Distance from Hoop")
plot(drive_data$game$lavine_speed, type = "l",
     xlab = "Time (25hz)", ylab = "Speed")
```

The speed metric is pretty noisy. This may be attributed to the fact that time is measured in 25 hertz (25ths of a second) and that location is determined by a computer vision algorithm and not a tracking chip attached to the player. They are many methods that can be used to smooth the data (e.g. splines). Here we use a basic rolling mean with a window of three time steps. In our example the data is not so noisy that it would affect the performance of our model so the smoothing is mostly for aesthetics and ease of interpretation. If the variance in the series was more extreme then we might want consider implementing a better smoothing method before fitting our model.

```{r fig.align='center', fig.height=8, fig.width=9}

lavine_speed_smooth <- rep(drive_data$game$lavine_speed[2],2)
for (i in 3:nrow(drive_data$game))
  lavine_speed_smooth[i] <- mean(drive_data$game$lavine_speed[(i-2):i], na.rm=TRUE)

drive_data <- readRDS("../data/evt140_0021500411.RDS")
par(mfrow = c(2,1))
plot(drive_data$game$lavine_speed, type = "l",
     main = "Raw Speed",
     xlab = "Time (25hz)", ylab = "Speed")
plot(lavine_speed_smooth, type = "l",
     main = "Smooth Speed",
     xlab = "Time (25hz)", ylab = "Speed")
```

Now that we have the data we can specify and fit the model.

### Specifying and Fitting the Model

Below is an graphic representation of the process that we are trying to model. 

```{stan eval=FALSE, output.var='drive_model.stan'}
data {

}
parameters {

}
model {

}
```

Below we fit the model to LaVine's data for the single drive event and inspect the traceplot of the parameter chains as a basic diagnostic check.

### Post-process

We post-process the state predictions and layer them on top of the original distance/speed plots to see if the predictions lined up with our logic.

We can also layer the predicted latent states on top of the previous video for a more comprehensive view. 

It looks like things line up nicely. The drive event gets triggered only when the player dramatically reduces their distance from the basket and increases their speed over time and

## Defensive Assignment

## Conclusion

Talk about how labeling the data would make the HMM parameters consistent with when the drive actually occurs.
Talk about pairing HMM with computer vision algorithms to apply meta data to the drive event regarding the type of shot (floater, bank, finger-roll, etc).
